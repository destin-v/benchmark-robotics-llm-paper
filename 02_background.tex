\section{Background}

In 2020, OpenAI released GPT-3\cite{brown2020languagemodelsfewshotlearners} which ignited considerable interest from the research community regarding Large Language Models (LLM).  In particular, robotics researchers began looking toward LLMs as a way of integrating intelligence into their automata.  Prior to the popularity of LLMs, most researchers were focused on designing robotic controls or leveraging reinforcement learning\cite{mu2021maniskillgeneralizablemanipulationskill} to perform robotic manipulation.  With the release of ever more powerful LLMs, robotics researchers began looking at ways to incorporate LLMs into their development.

In 2023, a global consortium of researchers from industry and academia released the Open-X\cite{embodimentcollaboration2024openxembodimentroboticlearning} dataset for robotic research. The Open-X dataset provided a dataset which could be used to train robotic manipulation with corresponding images and natural language instructions.  This paved the way for developing Vision Language Action (VLA) models which integrated language, vision, and robotic control.

In 2024, Berkley University released  Octo\cite{octomodelteam2024octoopensourcegeneralistrobot}; a seminal paper on using VLA Transformer to perform robotic manipulation.  In that same year OpenVLA\cite{kim2024openvlaopensourcevisionlanguageactionmodel} was released, demonstrating improved VLA performance built on top of Llama2\cite{touvron2023llama2openfoundation}.  Diffusion methods such as RDT-1B\cite{liu2025rdt1bdiffusionfoundationmodel} further improved upon OpenVLA to demonstrate additional performance gains under certain situations.

In parallel to robotic research, the number of simulators supporting robotic research was growing as well.  Popular robotic simulators have emerged such as Maniskill\cite{mu2021maniskillgeneralizablemanipulationskill}, Nvidia Isaac Sim\cite{mittal2023orbit}, AI2Thor\cite{kolve2022ai2thorinteractive3denvironment}, Robocasa\cite{robocasa2024} and Genesis\cite{Genesis}.  Many of these simulators were designed to support robotic manipulation by rendering physics accurately.

In 2024, Meta released a benchmark designed specifically to study embodied robotics in collaborative environments with explicit LLM support.  The benchmark was called Planning and Reasoning in Embodied Multi-agent Tasks (PARTNR)\cite{chang2024partnrbenchmarkplanningreasoning}; it was designed to study robot coordination using Large Language Models (LLMs) for indoor scenes.  PARTNR was built on the Habitat\cite{savva2019habitatplatformembodiedai} simulator and was specifically designed to benchmark heterogeneous multiagent coordination using LLMs:

\epigraph{PARTNR tasks exhibit characteristics of everyday tasks, such as spatial, temporal, and heterogeneous agent capability constraints. We employ a semi-automated task generation pipeline using Large Language Models (LLMs), incorporating simulation in the loop for grounding and verification. PARTNR stands as the largest benchmark of its kind, comprising 100,000 natural language tasks, spanning 60 houses and 5,819 unique objects.}{PARTNR: \\A Benchmark for Planning and Reasoning \\in Embodied Multi-agent Tasks}

PARTNR provides a standardized baseline for evaluating \textit{planning}, \textit{coordination}, and \textit{perception} for multiagent embodied robots in simulated indoor settings.  The tasks require \textit{planning} because agents must apply the correct sequence of actions in order to accomplish their tasks.  PARTNR simulates multiagent collaborative tasks which require \textit{coordination}.  Environments can be setup as either fully observable or partially observable to test the \textit{perception} capabilities of LLMs.  Tasks and environments are procedurally generated ensuring a diverse range of unique tests.

Despite state-of-the art performance of LLMs in advance math\cite{liu2024mathbenchevaluatingtheoryapplication}, graduate level questions\cite{rein2023gpqagraduatelevelgoogleproofqa}, and coding capabilities\cite{deepseekai2025deepseekr1incentivizingreasoningcapability,deepseekai2025deepseekv3technicalreport,openai2024openaio1card,openai2024gpt4ocard}; LLMs lack an embodied understanding of the physical world.  This gap between LLM's understanding of the physical world prevents generalized deployment of robotics into the real world.  In the PARTNR\cite{chang2024partnrbenchmarkplanningreasoning} paper they stated that state-of-the art LLM models displayed poor performance in embodied tasks:

\epigraph{
    The analysis reveals significant limitations in SoTA models, such as poor coordination and failures in task tracking and recovery from errors\dots  Overall, PARTNR highlights significant challenges facing collaborative embodied
    agents and aims to drive research in this direction\dots}{PARTNR: \\A Benchmark for Planning and Reasoning \\in Embodied Multi-agent Tasks}

Since the release of the original PARTNR benchmark, multiple new frontier models have been released such as OpenAI GPT-4o, OpenAI o3-mini, DeepSeek R1, etc.  OpenAI GPT-4o is considered a non-reasoning model whereas DeepSeek R1 and OpenAI o3-mini are considered reasoning models.  The purpose of our study was to determine how well these models perform on the PARTNR benchmark.  Chain of Thought (CoT), used in reasoning models, has shown dramatically improved performance over non-reasoning models using test time compute.  However, they utilize significantly more tokens and often take longer to produce an answer.  We wanted to understand whether reasoning models that utilized CoT could help with embodied tasks in robotics.



\section{Conclusion}

Our experiments showed that reasoning models such as o3-mini outperformed non-reasoning models like Llama 3 and GPT-4o.  While there was quite a bit of variation among the test points, o3-mini slightly outperformed the non-reasoning models in all test cases.  The out performance is statistically significant but the degree of outperform is small in each test case.

We identified and categorized multiple areas where frontier models struggled.  The problematic areas include: order of operations, spatial understanding, syntax, suboptimal planning, partial observability, and early stopping.  While humans sometimes struggle with these problems on an initial play through of an episode, humans quickly adapt and reach a near optimal solution.  LLMs often struggled with embodied robotic problems.  This is not surprising because most LLMs were trained on text scraped from the internet.  Most internet text does not require an embodied understanding grounded in physical reality.  PARTNR allows us to measure the knowledge gap of LLMs understanding of embodied indoor scenes.

Further questions that remain unresolved are whether more advance frontier reasoning models could improve performance.  Future options for research include testing Alibaba QwQ 32B or DeepSeek R1 which are considered state-of-the art open source reasoning models.  Google Gemini robotics\cite{geminiroboticsteam2025geminiroboticsbringingai} has started developing LLMs that have a better grounded understanding in embodied robotics.  This could be a promising avenue for research for robotics and LLMs.  In our testing, we only used text input LLMs, meaning we did not feed images or video to the LLMs.  Future research vectors can look at feeding image or video to VLA models.

In conclusion, leveraging LLMs for embodied robotics is difficult because models trained on text scraped from the internet do not directly translate into an understanding of physical spatial reality.  We believe that providing an embodied understanding of the physical world to LLMs would require a different type of training data.  There has already been some work in building datasets for embodied robotics such as the Open-X\cite{embodimentcollaboration2024openxembodimentroboticlearning} dataset.  This has led to new models like Google Gemini robotics.  Currently, Google Gemini has not released their models to the public, but it could be an interesting research direction for evaluation in the future.